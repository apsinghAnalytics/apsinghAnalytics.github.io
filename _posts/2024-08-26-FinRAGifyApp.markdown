---
layout: post
title: Building FinRAGify- An End-to-End Earnings Call Research Tool using RAG with LLMs  
image: finragify_UI.gif
date: 2024-08-26 9:39:20 +0400
tags: [Retrieval Augmented Generation (RAG), Large Language Model (LLM), Natural Language Processing, GPT-4o-Mini, LangChain, Prompt Engineering, Pretrained Language Model, Fine-Tuning, Question Answering System, Vector Embeddings, FAISS (Facebook AI Similarity Search), Vector Database, Hugging Face Models, Reranking, Finance, Stocks, AWS-EC2, Cloud Application, Docker Container]
categories: [App, LLM, Businesses, Data_Science, Finance]
---
Checkout the deployed app in action: [ http://ec2-40-177-46-181.ca-west-1.compute.amazonaws.com:8501]( http://ec2-40-177-46-181.ca-west-1.compute.amazonaws.com:8501) 

The link to the github repo for this app is [here](https://github.com/apsinghAnalytics/FinRAGify_App). Please check the readme in this repository for deployment instructions of this app.


---

#### **Introduction**

In the financial world, the ability to efficiently analyze company performance is crucial. Whether it's assessing management consistency or understanding market responses, a reliable tool to sift through vast amounts of earnings call data is invaluable. Enter **FinRAGify**—a proof of concept app designed to automate the analysis of earnings call transcripts using cutting-edge technologies like Retrieval-Augmented Generation (RAG), LangChain, and OpenAI's GPT-4 mini.

This blog will guide you through the development of FinRAGify, showcasing how advanced natural language processing (NLP) techniques can streamline financial analysis. We'll also discuss the two versions of the app, one using a Cross Encoder Model from Hugging Face for reranking, and the other, a lightweight version, utilizing Cohere's Reranking API.

<p align="center"> <img width="800" src="{{ site.baseurl }}/images/finragify_UI.gif"> </p>

---

### **The Problem Statement**

Financial analysts often face the daunting task of reviewing quarterly earnings for multiple companies. Manually sifting through transcripts to extract key insights and assess management’s consistency over time is both time-consuming and prone to human error. 

**FinRAGify** automates this process, allowing analysts to focus on decision-making rather than data gathering. By leveraging advanced NLP techniques, FinRAGify analyzes, ranks, and retrieves relevant information from large datasets, ensuring users get precise and contextually accurate answers swiftly.

---

### **Technical Overview**

FinRAGify is built using the following technologies:

- **LangChain**: A framework for building language model applications, enabling seamless integration of various NLP components.
- **OpenAI GPT-4 mini**: A language model that powers the app’s natural language understanding and generation.
- **FAISS (Facebook AI Similarity Search) Vector Database**: A vector database used to store and retrieve embeddings, allowing for rapid and relevant searches.
- **Streamlit**: A framework for building and deploying the app with a user-friendly interface.
- **FinancialModelingPrep API**: Used to gather the quarterly earning's call transcript data, ensuring up-to-date and reliable financial information.

---

### **Two Versions of FinRAGify**

**1. Regular Version:**
   - Uses an open-source **Cross Encoder Model** from [Hugging Face](https://huggingface.co/), a leading platform providing pre-trained models. This version reranks retrieved documents and requires about **300-600 MB of RAM**.
   - The Cross Encoder model, specifically **ms-marco-MiniLM-L-6-v2**, is pre-trained on millions of search data points, making it ideal for applications that require ranking multiple text snippets based on their relevance to a given query.

**2. Lightweight Version:**
   - Utilizes **Cohere's Reranking API** for document reranking, significantly reducing the RAM requirement to **150-300 MB**.
   - This version is optimal for deployment in environments with limited resources (e.g. free tier AWS EC2 t3.micro instance), offering a balance between performance and efficiency.

---

### **What is Retrieval-Augmented Generation (RAG) Technology?**

**Retrieval-Augmented Generation (RAG)** is an advanced AI technique that combines two powerful approaches: information retrieval and natural language generation. The core idea behind RAG is to enhance the output of a language model by grounding it in relevant, pre-existing data. This is done by retrieving pertinent documents or chunks of information from a database and then using these as context for the language model to generate more accurate and contextually relevant responses.

**How RAG Solves the Problem in FinRAGify:**

In the context of FinRAGify, RAG plays a crucial role in ensuring that the answers generated by the AI are not only coherent but also grounded in the actual content of the company's earnings call transcripts. Here's how it works:

1. **Information Retrieval**: When a user asks a question, FinRAGify first retrieves the most relevant chunks of data from the earnings call transcripts stored in the FAISS vector database. This retrieval is based on the similarity between the user's query and the content of the transcripts, ensuring that only the most relevant pieces of information are considered.

2. **Contextual Generation**: Once the relevant chunks are retrieved, they are provided as context to the language model (GPT-4 mini in this case). The model then generates a response that is directly informed by this context, leading to more accurate and reliable answers.

By combining retrieval with generation, RAG technology allows FinRAGify to offer detailed, contextually accurate insights while minimizing the risks of generating irrelevant or incorrect information. This approach is particularly effective in applications like financial analysis, where accuracy and relevance are paramount.



### **Breaking Down the Code: Recursive Text Splitting and Vector Storage**

To handle large transcripts effectively, FinRAGify uses a **Recursive Text Splitter** to break down the transcripts into manageable chunks of 700 characters each. Here’s how it works:

{% highlight python %}
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=700,
    chunk_overlap=0,
    length_function=len,
    is_separator_regex=False,
)
    
docs = text_splitter.create_documents(transcript_contents, metadatas=transcript_metadata)
{% endhighlight %}

The **Recursive Text Splitter** ensures that each chunk is within a specified size limit, preventing large blocks of text from overwhelming the model. This process is essential for creating structured data that can be fed into the model efficiently. The `create_documents` function then converts these chunks into LangChain document types, attaching relevant metadata like the **date, year, and quarter** of the earnings call.

These documents are then embedded using **OpenAI Embeddings**, and stored in a **FAISS** vector store. FAISS allows for quick similarity searches, making it easier to retrieve relevant transcript chunks when a question is asked.

{% highlight python %}
embeddings = OpenAIEmbeddings()
vectorstore_openai = FAISS.from_documents(docs, embeddings)
{% endhighlight %}

**Placeholder Image Suggestion**: A diagram showing how the text is split into chunks, embedded, and stored in the FAISS vector database.

---

### **How FinRAGify Works**

#### **Prompt Template and Document Retrieval**

The heart of FinRAGify lies in how it retrieves and processes data to generate insightful answers. The `get_answers` function is pivotal in this process:

{% highlight python %}
prompt_template = """You are a financial analyst. Using the following context from earnings call transcripts, answer the question below. Each paragraph includes details about the quarter and year, which helps establish the chronological order of the information. 

    Keep your answer concise, under 200 words.

    Context: {context}

    Question: {question}

    Answer:"""

prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
chain = prompt | chatLLM | StrOutputParser()
{% endhighlight %}

**Prompt Template**: This template assigns a specific role—here, that of a financial analyst—to focus the language model's responses on financial analysis. It handles custom questions that may not directly relate to company performance by ensuring that the context provided is relevant and concise.

- **Context**: Pulled from the data stored in the vector database, where the transcripts were broken into 700-character chunks.
- **Question**: The user’s query that triggered the retrieval of these chunks.

The retrieval process begins with the **FAISS similarity search**:

{% highlight python %}
retriever = vectorstore_openai.as_retriever(search_kwargs={"k": 25})
docs = retriever.invoke(question)
{% endhighlight %}

Here, `k=25` specifies the number of chunks retrieved based on similarity to the question. These chunks are then reranked using the Cross Encoder model, with the top 5 most relevant chunks provided to the LLM for final analysis.

**Placeholder Image Suggestion**: A flowchart illustrating the retrieval process, from question input to chunk retrieval, reranking, and final LLM response generation.

#### **Reranking: Why Use a Cross Encoder?**

The Cross Encoder model is particularly suited for this application because it evaluates pairs of text (the question and each chunk) to determine relevance, making it more accurate for ranking purposes. The model used here is pre-trained on millions of search queries, enhancing its ability to discern the most pertinent information from large datasets.

---

### **Ensuring Management Consistency: The check_management_consistency Function**

Similar to the `get_answers` function, the `check_management_consistency` function evaluates how consistent a company’s management has been in delivering on their promises. This function uses a fixed question within the prompt template, which is specifically designed to check for consistency over multiple quarters.

{% highlight python %}
retriever_search_qs = """What were the specific targets, deadlines, or expectations—such as those to be met by a certain quarter—in areas of product launches, strategic initiatives, cost-cutting measures, growth in new markets, share buybacks etc., that the management set to deliver on future quarters, and have they delivered on them?"""
{% endhighlight %}

By fine-tuning the retrieval query (e.g., removing the word "outlook"), the accuracy of retrieved chunks can be significantly improved, leading to more relevant responses and higher retrieval accuracy.

> **Prompt A**: "Review and summarize the company's planned initiatives ***and outlook*** for the upcoming quarters, focusing on strategic goals and future developments, for example, in areas of new markets, new products, new services, share buybacks."

> **Prompt B (fine-tuned)**: "Review and summarize the company's planned initiatives for the upcoming quarters, focusing on strategic goals and future developments, for example, in areas of new markets, new products, new services, share buybacks."

**Retrieval Accuracy**: This is calculated as the ratio of relevant chunks retrieved to the total chunks retrieved. By iteratively fine-tuning prompts, we achieved an average retrieval accuracy of **83%** for various pre-set questions across different stocks.

**Placeholder Image Suggestion**: A table comparing the retrieval accuracy of different prompts across multiple stocks.

---

### **Conclusion**

**FinRAGify** represents a significant step forward in how financial analysts can utilize AI to streamline their workflows. By incorporating both regular and lightweight versions, the tool offers flexibility in resource usage while maintaining high accuracy in financial analysis.

Whether you're using the Cross Encoder Model from Hugging Face for high-resource environments or Cohere's Reranking API for a more lightweight approach, FinRAGify ensures that you get accurate, contextually relevant answers to your financial questions. This tool not only saves time but also provides a level of analysis that can enhance decision-making processes in the financial sector.

Explore the code and learn more about FinRAGify on [GitHub](https://github.com/apsinghAnalytics/FinRAGify_App). Your feedback and contributions are always welcome as we continue to refine and expand this powerful tool.

---

Feel free to reach out with any questions or suggestions. Let's advance financial analysis together with AI!
